{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from math import floor\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "import utils\n",
    "# try:\n",
    "#     from neoConnector import all_cells_with_n_hops_in_area, get_all_edges\n",
    "# except ImportError:\n",
    "#     from .neoConnector import all_cells_with_n_hops_in_area, get_all_edges\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "class_map = {0: 'AMBIGUOUS', 1: 'nonTIL_stromal', 2: 'other', 3: 'sTIL', 4: 'tumor'}\n",
    "\n",
    "\n",
    "class BCSSGraphDatasetSUBGCN(Dataset):\n",
    "\n",
    "    def __init__(self, path, mode='train',\n",
    "                 num_layers=2,\n",
    "                 data_split=[0.8, 0.2], add_self_edges=False):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        path : list\n",
    "            List with filename, coordinates and path to annotation. For example, ['P7_HE_Default_Extended_1_1', (0, 2000, 0, 2000), 'datasets/annotations/P7_annotated/P7_HE_Default_Extended_1_1.txt']\n",
    "        mode : str\n",
    "            One of train, val or test. Default: train.\n",
    "        num_layers : int\n",
    "            Number of layers in the computation graph. Default: 2.\n",
    "        data_split: list\n",
    "            Fraction of edges to use for graph construction / train / val / test. Default: [0.85, 0.08, 0.02, 0.03].\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.path = path\n",
    "        self.mode = mode\n",
    "        self.num_layers = num_layers\n",
    "        self.data_split = data_split\n",
    "\n",
    "        print('--------------------------------')\n",
    "        print('Reading edge dataset from {}'.format(self.path[0]))\n",
    "\n",
    "        ########## MINE ###########\n",
    "        # Cells, distance_close_to_edges\n",
    "        edge_path = path[1]\n",
    "        node_path = path[2]\n",
    "    \n",
    "\n",
    "        # with glob\n",
    "        edges = pd.read_csv(edge_path, dtype={'source': np.int16, 'target': np.int16, 'distance': np.float16})\n",
    "        nodes = pd.read_csv(node_path)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "        #edges[\"type\"] = 0\n",
    "\n",
    "        #edges_crossing = edges.copy()\n",
    "        #edges_crossing = edges[edges[\"type\"] == 1]\n",
    "\n",
    "        #edges['type'] = edges['type'].replace(1, 0)\n",
    "\n",
    "        col_row_len = len(nodes['id'])\n",
    "        \n",
    "         \n",
    "\n",
    "       # Initialize a Numpy array for better performance\n",
    "        distances_close_to_edges = np.zeros((col_row_len, col_row_len))\n",
    "\n",
    "\n",
    "        #distances_close_to_edges = pd.DataFrame(0, index=np.arange(col_row_len), columns=np.arange(col_row_len))\n",
    "        #delta_entropy_edges = pd.DataFrame(0, index=np.arange(col_row_len), columns=np.arange(col_row_len))\n",
    "        #neighborhood_similarity_edges = pd.DataFrame(0, index=np.arange(col_row_len), columns=np.arange(col_row_len))\n",
    "\n",
    "        for i, row in edges.iterrows():\n",
    "            #source = row['source']\n",
    "            #target = row['target']\n",
    "\n",
    "            distances_close_to_edges[int(row['source']), int(row['target'])] = float(row['distance'])\n",
    "            distances_close_to_edges[int(row['target']), int(row['source'])] = float(row['distance'])\n",
    "            \n",
    "            #distance = float(row['distance'])\n",
    "           \n",
    "            #delta_entropy = float(row['Delta_Entropy'])\n",
    "            #sorenson_neigh_similarity = float(row['Sorenson_Similarity'])\n",
    "        \n",
    "            #distances_close_to_edges[source][target] = distance\n",
    "            #distances_close_to_edges[target][source] = distance\n",
    "\n",
    "            #delta_entropy_edges[source][target] = delta_entropy\n",
    "            #delta_entropy_edges[target][source] = delta_entropy\n",
    "\n",
    "            #neighborhood_similarity_edges[source][target] = sorenson_neigh_similarity\n",
    "            #neighborhood_similarity_edges[target][source] = sorenson_neigh_similarity\n",
    "\n",
    "        \n",
    "        # Convert the Numpy array back to a DataFrame if needed\n",
    "        #distances_close_to_edges = pd.DataFrame(distance_matrix)\n",
    "        #distances_close_to_edges = distance_matrix#np.array(distances_close_to_edges)\n",
    "        #delta_entropy_edges = np.array(delta_entropy_edges)\n",
    "        #neighborhood_similarity_edges = np.array(neighborhood_similarity_edges)\n",
    "\n",
    "        # coords\n",
    "        coords = nodes[[\"x\", \"y\"]].to_numpy()\n",
    "\n",
    "        # process neighborhood densities\n",
    "        #density_types = [\"Cell_density\"]\n",
    "        #entropy_types = [\"Node_Entropy\"]\n",
    "\n",
    "        #densities = nodes[density_types].to_numpy()\n",
    "        #edge_density = np.zeros((col_row_len, col_row_len))\n",
    "        #edge_densities = np.empty((0, col_row_len, col_row_len))\n",
    "\n",
    "        \n",
    "        # for i in range(len(density_types)):\n",
    "        #     for _, row in edges.iterrows():\n",
    "        #         source = int(row['source'])\n",
    "        #         target = int(row['target'])\n",
    "\n",
    "        #         edge_density[source][target] = float(densities[:, i][target]) - float(densities[:, i][source])\n",
    "        #         edge_density[target][source] = float(densities[:, i][source]) - float(densities[:, i][target])\n",
    "\n",
    "        #     edge_densities = np.append(edge_densities, edge_density.reshape(-1, col_row_len, col_row_len), axis=0)\n",
    "\n",
    "        #print('*************')\n",
    "        #print('Edge_density Shape : ' + str(edge_densities.shape))\n",
    "        \n",
    "        distances_close_to_edges = distances_close_to_edges.reshape(-1, col_row_len, col_row_len)\n",
    "        #delta_entropy_edges = delta_entropy_edges.reshape(-1, col_row_len, col_row_len)\n",
    "        #neighborhood_similarity_edges = neighborhood_similarity_edges.reshape(-1, col_row_len, col_row_len)\n",
    "\n",
    "        #print('Edge_entropy Shape : ' + str(delta_entropy_edges.shape))\n",
    "        #print('Edge_distance Shape : ' + str(distances_close_to_edges.shape))\n",
    "        #print('Neighborhood Similarity Shape : ' + str(neighborhood_similarity_edges.shape))\n",
    "        #edge_features = np.concatenate((edge_densities, delta_entropy_edges, neighborhood_similarity_edges, distances_close_to_edges), axis=0)\n",
    "         \n",
    "        #edge_features = distances_close_to_edges.reshape(-1, col_row_len, col_row_len)\n",
    "        #print(edge_features)\n",
    "        # self.edge_features = utils.normalize_edge_feature_doubly_stochastic(edge_features) ### not to be used\n",
    "        self.edge_features = utils.normalize_edge_features_rows(distances_close_to_edges) ### Use it to normalise the edge features\n",
    "        #self.edge_features = edge_features  ### Use only if not using the normalization feature above\n",
    "\n",
    "\n",
    "        ## To DO \n",
    "\n",
    "        # Change utils.normalize_edge_features_rows to log function to the base e\n",
    "\n",
    "        #####\n",
    "\n",
    "        self.channel = distances_close_to_edges.shape[0]\n",
    "\n",
    "        self.dist = self.edge_features#utils.normalize_edge_features_rows(distances_close_to_edges.reshape(-1, col_row_len, col_row_len))\n",
    "\n",
    "\n",
    "        # all_labels_cell_types\n",
    "        #nodes[\"gt\"].replace({'AMBIGUOUS': 0, 'nonTIL_stromal': 1, 'other': 2, 'sTIL': 3, 'tumor': 4}, inplace=True) # hover-net\n",
    "                     \n",
    "\n",
    "        # nuclei features\n",
    "        #nuclei_feat = nodes[[\"area\", \"perim\"]].to_numpy()\n",
    "\n",
    "        #all_labels_cell_types = nodes[\"gt\"].to_numpy()\n",
    "        all_labels_cell_types = nodes[\"class\"].to_numpy()\n",
    "\n",
    "        # One-hot encoding for class types\n",
    "        class_columns = ['amb', 'nonTil_stroma', 'other', 'sTIL', 'tumor']\n",
    "\n",
    "        # Create dummy variables\n",
    "        nodes_with_types_zero_one = pd.get_dummies(nodes['gt'], prefix='', prefix_sep='')\n",
    "\n",
    "        # Reindex the DataFrame to include all columns in 'class_columns', filling missing ones with 0s\n",
    "        nodes_with_types_zero_one = nodes_with_types_zero_one.reindex(columns=class_columns, fill_value=0)\n",
    "        # Assigning y_nodes based on 'mask' values\n",
    "        y_nodes = np.where(nodes['mask'].isin([1, 19, 20]), 1, 0)\n",
    "\n",
    "        #y_class_columns = ['tumor', 'stroma', 'inflammatory', 'necrosis', 'other']\n",
    "        # Tumor 1 19 20\n",
    "        # Stroma 2\n",
    "        # Inflammatory 3 10 11\n",
    "        # Necrosis 4\n",
    "        # Other 5 6 7 8 9 12 13 14 15 16 17 18 21\n",
    "        # y_mapping = {0: 'other', 1: 'tumor', 2: 'stroma', 3: 'inflammatory', \n",
    "        #              4: 'necrosis', 5: 'other', 6: 'other', 7: 'other', 8: 'other', \n",
    "        #              9: 'other', 10: 'inflammatory', 11: 'inflammatory', 12: 'other', \n",
    "        #              13: 'other', 14: 'other', 15: 'other', 16: 'other', 17: 'other', \n",
    "        #              18: 'other', 19: 'tumor', 20: 'tumor', 21: 'other'\n",
    "        #         }\n",
    "        # nodes['y_mask'] = nodes['mask'].map(y_mapping)\n",
    "        # y_one_hot_encoding = pd.get_dummies(nodes['y_mask'], prefix='', prefix_sep='')\n",
    "        # y_one_hot_encoding = y_one_hot_encoding.reindex(columns=y_class_columns, fill_value=0)\n",
    "        # # count number of nodes in each class\n",
    "        # print('Number of nodes in each class:')\n",
    "        # print(y_one_hot_encoding.sum(axis=0))\n",
    "\n",
    "        \n",
    "        # x_nodes as 'id' column from nodes DataFrame\n",
    "        x_nodes = nodes['id'].to_numpy()\n",
    "\n",
    "        # Count positives and negatives\n",
    "        y_pos, y_neg = np.count_nonzero(y_nodes == 1), np.count_nonzero(y_nodes == 0)\n",
    "\n",
    "        # Extract cell type scores\n",
    "        cell_types_scores = nodes_with_types_zero_one.to_numpy()\n",
    "        #print(cell_types_scores.shape)\n",
    "\n",
    "        # adjacency_matrix_close_to_edges\n",
    "        adjacency_matrix_close_to_edges = np.copy(distances_close_to_edges)\n",
    "        adjacency_matrix_close_to_edges[adjacency_matrix_close_to_edges != 0] = 1\n",
    "        self.adj = adjacency_matrix_close_to_edges\n",
    "        # edge_list_close_to_edge\n",
    "        #edge_list_close_to_edge = edges[[\"source\", \"target\"]]\n",
    "        #edge_list_close_to_edge = edge_list_close_to_edge.to_numpy()\n",
    "\n",
    "        # edge_list_crossing_edges\n",
    "        # edge_list_crossing_edges = edges_crossing.to_numpy()\n",
    "\n",
    "        #self.am_close_to_edges_including_distances = distances_close_to_edges\n",
    "        self.classes = all_labels_cell_types\n",
    "        self.class_scores = cell_types_scores\n",
    "        self.coords = coords\n",
    "\n",
    "        print('Finished reading data.')\n",
    "\n",
    "        print('Setting up graph.') \n",
    "        self.nodes_count = len(coords) # Count vertices\n",
    "        self.edges_count = len(edges) # Count edges\n",
    "\n",
    "        self.features = torch.from_numpy(cell_types_scores).float()  # Cell features with just one-hot encoding \n",
    "           \n",
    "        \n",
    "        print('self.features.shape:', self.features.shape)\n",
    "        # [2] end\n",
    "\n",
    "        print('Finished setting up graph.')\n",
    "\n",
    "        print('Setting up examples.')\n",
    "\n",
    "\n",
    "        # # Assuming y_nodes contain labels for each node (1 for positive, 0 for negative)\n",
    "        # positive_node_indices = [i for i, label in enumerate(y_nodes) if label == 1]\n",
    "        # negative_node_indices = [i for i, label in enumerate(y_nodes) if label == 0]\n",
    "\n",
    "        # # Generate negative and positive examples\n",
    "        # positive_examples = []\n",
    "        # negative_examples = []\n",
    "        # _choice = np.random.choice\n",
    "\n",
    "        # if self.mode != 'train':  # Validation/Test Mode\n",
    "        #     print(\"self.mode != 'train'\")\n",
    "        #     # In this mode, you might want to handle positive and negative examples differently\n",
    "        #     # Adjust the following logic as per your validation/test requirements\n",
    "        #     positive_examples = positive_examples + positive_node_indices\n",
    "        #     negative_examples = negative_examples + negative_node_indices\n",
    "        # else:  # Training Mode\n",
    "        #     # For training, you might want to balance the number of positive and negative examples\n",
    "        #     num_examples = min(len(positive_node_indices), len(negative_node_indices))\n",
    "\n",
    "        #     positive_examples = _choice(positive_node_indices, num_examples, replace=False)\n",
    "        #     negative_examples = _choice(negative_node_indices, num_examples, replace=False)\n",
    "\n",
    "        # # Convert to numpy arrays\n",
    "        # positive_examples = np.array(positive_examples, dtype=np.int64)\n",
    "        # negative_examples = np.array(negative_examples, dtype=np.int64)\n",
    "\n",
    "        # print('y_pos, y_neg', y_pos, y_neg)\n",
    "        # SMTH not working with sampling\n",
    "        x = x_nodes#np.vstack((positive_examples, negative_examples))\n",
    "        y = y_nodes\n",
    "  \n",
    "        #print(y)\n",
    "        perm = np.random.permutation(x.shape[0])\n",
    "        x, y = x[perm], y[perm]  # ERROR HERE -> IndexError: too many indices for array: array is 1-dimensional,\n",
    "        # but 2 were indexed\n",
    "        x, y = torch.from_numpy(x).long(), torch.from_numpy(y).long()\n",
    "        self.x, self.y = x, y\n",
    "\n",
    "        print('Finished setting up examples.')\n",
    "\n",
    "        print('Dataset properties:')\n",
    "        print('Mode: {}'.format(self.mode))\n",
    "        print('Number of vertices: {}'.format(self.nodes_count))\n",
    "        print('Number of edges: {}'.format(self.edges_count))\n",
    "        \n",
    "        # print('Number of positive/negative nodes: {}/{}'.format(positive_examples.shape[0],\n",
    "        #                                                             negative_examples.shape[0]))\n",
    "        print('Number of tumor/non-tumor: {}/{}'.format(y_pos, y_neg))\n",
    "        print('Number of examples/datapoints: {}'.format(self.x.shape[0]))\n",
    "\n",
    "        print('--------------------------------')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "    def get_coords_and_class(self):\n",
    "        return self.coords, self.classes\n",
    "\n",
    "    # def _form_computation_graph(self, idx):\n",
    "    #     \"\"\"\n",
    "    #     Parameters\n",
    "    #     ----------\n",
    "    #     idx : int or list\n",
    "    #         Indices of the node for which the forward pass needs to be computed.\n",
    "    #     Returns\n",
    "    #     -------\n",
    "    #     node_layers : list of numpy array\n",
    "    #         node_layers[i] is an array of the nodes in the ith layer of the\n",
    "    #         computation graph.\n",
    "    #     mappings : list of dictionary\n",
    "    #         mappings[i] is a dictionary mapping node v (labelled 0 to |V|-1)\n",
    "    #         in node_layers[i] to its position in node_layers[i]. For example,\n",
    "    #         if node_layers[i] = [2,5], then mappings[i][2] = 0 and\n",
    "    #         mappings[i][5] = 1.\n",
    "    #     \"\"\"\n",
    "    #     _list, _set = list, set\n",
    "    #     if type(idx) is int:\n",
    "    #         node_layers = [np.array([idx], dtype=np.int64)]\n",
    "    #     elif type(idx) is list:\n",
    "    #         node_layers = [np.array(idx, dtype=np.int64)]\n",
    "\n",
    "    #     for _ in range(self.num_layers):\n",
    "    #         prev = node_layers[-1]\n",
    "    #         arr = [node for node in prev]\n",
    "    #         arr.extend([e for node in arr for e in self.node_neighbors[node]])  # add neighbors to graph\n",
    "    #         arr = np.array(_list(_set(arr)), dtype=np.int64)\n",
    "    #         node_layers.append(arr)\n",
    "    #     node_layers.reverse()\n",
    "\n",
    "    #     mappings = [{j: i for (i, j) in enumerate(arr)} for arr in node_layers]\n",
    "\n",
    "    #     return node_layers, mappings\n",
    "\n",
    "    def collate_wrapper(self, batch):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch : list\n",
    "            A list of examples from this dataset. An example is (node, label).\n",
    "        Returns\n",
    "        -------\n",
    "        adj : torch.Tensor\n",
    "            adjacency matrix of entire graph\n",
    "        features : torch.FloatTensor\n",
    "            A (n' x input_dim) tensor of input node features.\n",
    "        edge_features : torch.FloatTensor\n",
    "            A 3d tensor of edge features.\n",
    "        edges : numpy array\n",
    "            The edges in the batch.\n",
    "        labels : torch.LongTensor\n",
    "            Labels (1 or 0) for the edges in the batch.\n",
    "        dist : torch.Tensor\n",
    "            A distance matrix\n",
    "        \"\"\" \n",
    "        adj = torch.from_numpy(self.adj).float()\n",
    "\n",
    "        features = self.features\n",
    "        edge_features = torch.from_numpy(self.edge_features).float()\n",
    "        #edges = np.array([sample[0].numpy() for sample in batch])\n",
    "        labels = torch.FloatTensor([sample[1] for sample in batch])\n",
    "        nodes = np.array([sample[0].numpy() for sample in batch])\n",
    "        #labels = torch.stack([sample[1] for sample in batch]).float()\n",
    "            \n",
    "\n",
    "\n",
    "        dist = torch.from_numpy(self.dist)\n",
    "\n",
    "        return adj, features, edge_features, nodes, labels, dist\n",
    "        #return features, edge_features, nodes, labels, dist\n",
    "\n",
    "    def get_dims(self):\n",
    "        print(\"self.features.shape: {}\".format(self.features.shape))\n",
    "        print(\"input_dims (input dimension) -> self.features.shape[1] = {}\".format(self.features.shape[1]))\n",
    "        return self.features.shape[1], 1\n",
    "\n",
    "    def get_channel(self):\n",
    "        return self.channel\n",
    "\n",
    "    def parse_points(self, fname):\n",
    "        with open(fname, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        lines = [line[:-1].split(',') for line in lines]  # Remove \\n from line\n",
    "        return lines\n",
    "##############################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "def line(p1, p2):\n",
    "    A = (p1[1] - p2[1])\n",
    "    B = (p2[0] - p1[0])\n",
    "    C = (p1[0]*p2[1] - p2[0]*p1[1])\n",
    "    return A, B, -C\n",
    "\n",
    "def intersection(L1, L2):\n",
    "    D  = L1[0] * L2[1] - L1[1] * L2[0]\n",
    "    Dx = L1[2] * L2[1] - L1[1] * L2[2]\n",
    "    if D != 0:\n",
    "        x = Dx / D\n",
    "        return x\n",
    "    else:\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "adjacency = np.array([[1,1,0,1,1],\n",
    "                      [1,1,1,0,0],\n",
    "                      [0,1,1,2,0],\n",
    "                      [1,0,1,1,1],\n",
    "                      [1,0,0,1,1]])\n",
    "\n",
    "def neighbours(adjacency, node):\n",
    "    # remove self loops\n",
    "    adjacency[node, node] = 0\n",
    "    return np.where(adjacency[node])[0]\n",
    "\n",
    "neighbours(adjacency, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def node_graphlets(features, nodes, adjacency, device=\"cpu\"):\n",
    "   \n",
    "    _u = torch.FloatTensor().to(device)\n",
    "   \n",
    "    _Nu = [torch.FloatTensor().to(device) for i in range(13)] #max degree\n",
    "    # must implement zero padding\n",
    "     \n",
    "    for u in nodes:\n",
    "    \n",
    "        neighbors_u = neighbours(adjacency, u)\n",
    "        _u = torch.cat((_u, features[u].reshape(1, -1)), dim=0)\n",
    "       \n",
    "        for i,neighbor in enumerate(neighbors_u):\n",
    "            \n",
    "            _Nu[i] = torch.cat((_Nu[i], features[neighbor].reshape(1, -1)), dim=0)\n",
    "            # fill if len(neighbors_u) < 13 with zeros\n",
    "        if len(neighbors_u) < 13:\n",
    "            for i in range(len(neighbors_u), 13):\n",
    "                _Nu[i] = torch.cat((_Nu[i], torch.zeros(1, features.shape[1]).to(device)), dim=0)\n",
    "\n",
    "                \n",
    "\n",
    "    input_data = torch.stack([_u,_Nu[0],_Nu[1],_Nu[2],_Nu[3],_Nu[4],_Nu[5],_Nu[6],_Nu[7],_Nu[8],_Nu[9],_Nu[10],_Nu[11],_Nu[12]], dim=1)\n",
    "    input_data = input_data.permute(0,1,2)#.view(-1, 4, 8, 16)\n",
    "    \n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Reading edge dataset from datasets/ground_truth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading data.\n",
      "Setting up graph.\n",
      "self.features.shape: torch.Size([2746, 5])\n",
      "Finished setting up graph.\n",
      "Setting up examples.\n",
      "Finished setting up examples.\n",
      "Dataset properties:\n",
      "Mode: train\n",
      "Number of vertices: 2746\n",
      "Number of edges: 8214\n",
      "Number of tumor/non-tumor: 789/1957\n",
      "Number of examples/datapoints: 2746\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "datasets = BCSSGraphDatasetSUBGCN(path=['datasets/ground_truth','datasets/tiles1/train/TCGA-A1-A0SK-DX1_xmin45749_ymin25055_MPP-0.2000_tile_0_0_edges.csv','datasets/tiles1/train/TCGA-A1-A0SK-DX1_xmin45749_ymin25055_MPP-0.2000_tile_0_0_nodes.csv'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj, features, edge_features, nodes, labels, dist = datasets.collate_wrapper([(torch.tensor(0), -11), (torch.tensor(1), 0), (torch.tensor(2), 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2746, 2746]),\n",
       " torch.Size([2746, 5]),\n",
       " torch.Size([1, 2746, 2746]),\n",
       " (3,),\n",
       " torch.Size([3]),\n",
       " torch.Size([1, 2746, 2746]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj.shape, features.shape, edge_features.shape, nodes.shape, labels.shape, dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "1 3\n",
      "2 4\n"
     ]
    }
   ],
   "source": [
    "neighbors_u = neighbours(adjacency, 0)\n",
    "\n",
    "for i,neighbor in enumerate(neighbors_u):\n",
    "    print(i, neighbor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphlets = node_graphlets(features, nodes, adj[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
